{{cookiecutter.project_name}}
==============================

{{cookiecutter.description}}

# ML Projects Readme Template

A simple and well designed structure is essential for any Machine Learning project, so we have designed a project template that combines **simplicity, best practice for folder structure** and **good OOP design** which could be easily scaled into any machine learning project.
The main idea is that there's much same stuff you do every time when you start your machine learning project, so wrapping all repetitive parts of documentation and delivery will help you to change just the core idea every time you start a new pytorch project. 

**So, hereâ€™s a simple template that help you get into your main project faster and just focus on your core (Model Architecture, Training Flow, etc)**

A few tips to keep in mind while using this template:
- You can remove everything above table of contents and populate project title and description if not done already.
- The current structure of readme is designed keeping in mind my past projects and my idea of ML projects. Hence, be flexible with content of readme. If a section doesn't make sense for your project, remove it. Likewise, if a section is missing from the template that you would think is necessary, add it.
- Try to not remove the "In a Nutshell", "Directory structure" "Contributing", "Acknowledgments" as these apply to all projects.


# Table Of Contents
-  [Paper Title](#paper-title)
-  [In a Nutshell](#in-a-nutshell)
-  [Directory Structure](#directory-structure)
-  [Requirements](#requirements)
-  [Training](#training)
-  [Evaluation](#evaluation)
-  [Pre-trained Models](#pre-trained-models)
-  [Results](#results)
-  [Future Work](#future-work)
-  [Contributing](#contributing)
-  [Acknowledgments](#acknowledgments)


## Paper Title
This repository is the official implementation of [Paper Title](https://arxiv.org/abs/2030.12345). 

>ðŸ“‹  Optional: include a graphic explaining your approach/main result, bibtex entry, link to demos, blog posts and tutorials

tip: use this section to link a paper if there exists or there is going to be a paper written for this project.

## In a Nutshell   
> ðŸ“‹  Use this section to give the audience a quick guide on setup instructions and running a version of your project.

Instructions could include things like:

- creating appropriate folders or filepaths

-- Example: In `modeling`  folder create a python file named whatever you like, here we named it `example_model.py` . In `modeling/__init__.py` file, you can build a function named `build_model` to call your model

- creating code blocks

Example: 
```python
from .example_model import ResNet18

def build_model(cfg):
    model = ResNet18(cfg.MODEL.NUM_CLASSES)
    return model
``` 

- instructions on how to install libraries and dependencies

## Directory structure
```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources.
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
â”‚
â”œâ”€â”€ documents               <- A default Sphinx project; see sphinx-doc.org for details
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`.
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
â”œâ”€â”€ src                <- Source code for use in this project.
â”‚   â”œâ”€â”€ __init__.py    <- Makes src a Python module
â”‚   â”‚
â”‚   â”œâ”€â”€ data           <- Scripts to download or generate data
â”‚   â”‚   â””â”€â”€ make_dataset.py
â”‚   â”‚
â”‚   â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
â”‚   â”‚   â”‚                 predictions
â”‚   â”‚   â”œâ”€â”€ predict_model.py
â”‚   â”‚   â””â”€â”€ train_model.py
â”‚   â”‚
â”‚   â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
â”‚       â””â”€â”€ visualize.py
```

>ðŸ“‹  This will be the actual file structure of your project which could look a bit different from the template directory structure

## Requirements
To install requirements:

```setup
pip install -r requirements.txt
```

>ðŸ“‹  Describe how to set up the environment, e.g. pip/conda/docker commands, download datasets, etc...

## Training
To train the model(s) in the paper, run this command:

```train
python train.py --input-data <path_to_data> --alpha 10 --beta 20
```

>ðŸ“‹  Describe how to train the models, with example commands on how to train the models in your paper, including the full training procedure and appropriate hyperparameters.

## Evaluation
To evaluate my model on example_dataset, run:

```eval
python eval.py --model-file mymodel.pth --benchmark example_dataset
```

>ðŸ“‹  Describe how to evaluate the trained models on benchmarks reported in the paper, give commands that produce the results (section below).

## Pre-trained Models
You can download pretrained models here:

- [My awesome model](https://drive.google.com/mymodel.pth) trained on <example_dataset> using parameters x,y,z. 

>ðŸ“‹  Give a link to where/how the pretrained models can be downloaded and how they were trained (if applicable).  Alternatively you can have an additional column in your results table with a link to the models.

## Results
Our model achieves the following performance on :

| Model name         | Top 1 Accuracy  | Top 5 Accuracy |
| ------------------ |---------------- | -------------- |
| My awesome model   |     85%         |      95%       |

>ðŸ“‹  Include a table of results from your paper, and link back to the leaderboard for clarity and context. If your main result is a figure, include that figure and link to the command or notebook to reproduce it. 

## Future work
Discuss the scope of future work and share any literature survey already done on the topic that lead to ideas for the scope.

## Contributing
Any kind of enhancement or contribution is welcomed.

>ðŸ“‹  Pick a licence and describe how to contribute to your code repository.

## Acknowledgments
Acknowledge people's contribution in the project.

--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
